forked from 4 idiots 

## read code for "./run.sh 0"
1. main logic is in base/run.py
2. factorization machine solver is in mark/mark1/mark1. compile it first for further use. there is a readme in the dir, explained the input data format
3. then generate data from train.csv
  ./util/gen_data.py ../tr.r0.csv ../va.r0.csv tr.r0.app.new.csv va.r0.app.new.csv tr.r0.site.new.csv va.r0.site.new.csv
  2 input data: tr.r0.csv. va.r0.csv.  tr.r0.csv is a symbol link of original train.csv.  va.r0.csv is the test data, adding a dummy label 0 to each line.
  (but test data has no label, how can it be a validation data?)
  there is 4 data output: train data and validation data. and divided to app data and site data
  let's see the generation detail
4. in gene_data, first scan data, record 4 dict of several value counts. then in gene_data, use the counts-dict as features.
5. in parallelizer.py , the main logic is in converter/2.py.  There are some logic of split data to 12 piece, using 12 thread to handle every piece. then merge the data.
  in converter/2.py, generate one-hot coding. but using hashstr to generate one hot coding, are there lots of missing index, and collision of hash value???
6. run mark1, this program do train and predict in one run. the predict output in xxx.prd.
7. there is a merge Prediction stage. the main logic is in :
   logistic_func(sum(map(inv_logistic_func, mprd[key]))/len(mprd[key]))
   this function may be used in bagging peroid. inverse the logistic function and get the average value of Prediction, then make it to logic again.
   in base ,len(mprd[key]) is always 1.
8. using run.sh 0 , submit the output base.r0.prd, get a score of 0.3832105, rank 10 th.


=======================================
improvement

1. directly implement the one hot encoding, using index ,not hash str. using all features and no feature engineering . 
   submit to Kaggle, the score is 0.4091047, rank 1000 +
2. a way to do feature engineering : if some one-hot feature is missing in validation or test set, there must have influence in model Prediction. there is some way to improve the feature. like:
   hour feature. just keep hour, delete date.  14123100 - > 00 . or generate several features, like, hour, weekday, weekend, holiday, etc.

     



4 Idiots' Approach for Click-through Rate Prediction
====================================================

Our team consists of:
    
    Name              Kaggle ID         Affiliation
    ====================================================================
    Yu-Chin Juan      guestwalk         National Taiwan University (NTU)
    Wei-Sheng Chin    mandora           National Taiwan University (NTU)
    Yong Zhuang       yolicat           National Taiwan University (NTU)
    Michael Jahrer    Michael Jahrer    Opera Solutions

Our final model is an ensemble of NTU's model and Michael's model. Michael's
model is based on his work in Opera Solutions, so he cannot release his part.
Therefore, in the codes and documents we only present NTU's model.

This README introduces how to run our code up. For the introduction to our
approach, please see 

    http://www.csie.ntu.edu.tw/~r01922136/slides/kaggle-avazu.pdf

The model we use for this competition is called `field-aware factorization
machines.' We have released a package for this model at:

    http://www.csie.ntu.edu.tw/~r01922136/libffm



System Requirement
==================

- 64-bit Unix-like operating system

- Python 3

- g++ (with C++11 and OpenMP support)

- pandas (required if you want to run the `bag' part. See `Step-by-step'
  below.)



Step-by-step
============

Our solution is an ensemble of 20 models. It is organized into the following
three parts:
    
    name       public score     private score     description        
    ===========================================================================
    base             0.3832            0.3813     2 basic models

    bag              0.3826            0.3807     2 models using bag features.

    ensemble         0.3817            0.3797     an ensemble of the above 4 
                                                  models and 16 new small models

Because the `bag' part consumes a huge amount of memory (more than 64GB), and
the `ensemble' part takes a long time to run, this instruction guides you to
run our `base' part first. If you want reproduce our best result, please run the
commands in the final step on a suitable machine.


1.  First, please use the following command to run a tiny example up

    $ ./run.sh x

2.  Create a symbolic link to the training dataset

    $ ln -sf <training_set_path> tr.r0.csv

3.  Add a dummy label to the test set

    $ ./add_dummy_label.py <test_set_path> va.r0.csv

4.  Checksum

    $ md5sum tr.r0.csv va.r0.csv
    f5d49ff28f41dc993b9ecb2372abb033  tr.r0.csv
    6edd380a5897bc16b61c5a626062f7b3  va.r0.csv

5.  Reproduce our base submission

    $ ./run.sh 0
    
    Note: base.r0.prd is the submission file

6.  (optional) Reproduce our best submission

    $ ./run_all.sh x

    If success, then run

    $ ./run_all.sh 0

    Note: The algorithm in the `bag' part is non-deterministic. That is, the
    result can be slightly different when you run it two or more times.



==============

If you want to trace these codes, please be prepared that it will take some
efforts. We do not have enough time to polish the codes here to improve the
readability. Sorry about it. 

For any questions and comments, please send your email to:

    Yu-Chin (guestwalk@gmail.com)
